{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì—¬'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from .bam import BAM\n",
    "from .cbam import CBAM\n",
    "from .conv import Conv2d\n",
    "\n",
    "\n",
    "\n",
    "class SyncNet_color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SyncNet_color, self).__init__()\n",
    "\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
    "        face_embedding = self.face_encoder(face_sequences)\n",
    "        audio_embedding = self.audio_encoder(audio_sequences)\n",
    "\n",
    "        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
    "        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
    "\n",
    "        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
    "        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "        return audio_embedding, face_embedding\n",
    "    \n",
    "    \n",
    "class _Syncnet_color_Attention(SyncNet_color):\n",
    "    attention_dict = {\n",
    "        'None': None,\n",
    "        'bam': BAM,\n",
    "        'cbam': CBAM,\n",
    "        'cbam_inter': CBAM\n",
    "    }\n",
    "    def __init__(self, face_attention_type=\"none\", audio_attention_type=\"none\"):\n",
    "        self.initialize()\n",
    "        self.face_attention_type = face_attention_type\n",
    "        self.audio_attention_type = audio_attention_type\n",
    "        \n",
    "    def initialize(self):\n",
    "        super(_Syncnet_color_Attention, self).__init__()\n",
    "        self.built = False\n",
    "        \n",
    "    def from_pretrained(self, syncnet96):\n",
    "        self.face_encdoer = syncnet96.face_encoder\n",
    "        self.audio_encoder = syncnet96.audio_encoder\n",
    "    \n",
    "    def build_model(self):\n",
    "        if not self.built:\n",
    "            self._build_model()\n",
    "            self.built=True\n",
    "        else:\n",
    "            print('model has already been built')\n",
    "            \n",
    "    def _build_model(self):\n",
    "        pass\n",
    "    \n",
    "    def add_attention(self, encoder, attention_type, attention_info):\n",
    "        add_len=0\n",
    "        for l, in_channels in attention_info.items():\n",
    "            add_len += 1\n",
    "            encoder = nn.Sequential(\n",
    "                *encoder[:l+add_len],\n",
    "                self.attention_dict[attention_type](in_channels),\n",
    "                *encoder[l+add_len:]\n",
    "            )\n",
    "            \n",
    "        return encoder\n",
    "        \n",
    "#     def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
    "#         # Encoding\n",
    "#         face_embedding = face_sequences\n",
    "#         for i in range(len(self.face_encoder)):\n",
    "#             face_embedding = self.face_encoder[i](face_embedding)\n",
    "#             if self.face_attention_type is not \"none\" and i in self.face_attentions.keys():\n",
    "#                 face_embedding = self.face_attentions[i](face_embedding)\n",
    "        \n",
    "#         audio_embedding = audio_sequences\n",
    "#         for i in range(len(self.audio_encoder)):\n",
    "#             audio_embedding = self.audio_encoder[i](audio_embedding)\n",
    "#             if self.audio_attention_type is not \"none\" and i in self.audio_attentions.keys():\n",
    "#                 audio_embedding = self.audio_attentions[i](audio_embedding)\n",
    "\n",
    "#         # Reshape        \n",
    "#         audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
    "#         face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
    "        \n",
    "#         # Normalize\n",
    "#         audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
    "#         face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
    "\n",
    "#         return audio_embedding, face_embedding\n",
    "        \n",
    "        \n",
    "class Syncnet_color_96(_Syncnet_color_Attention):\n",
    "    def __init__(self, face_attention_type=\"none\", audio_attention_type=\"none\", freeze=False):\n",
    "        super(Syncnet_color_96, self).__init__(face_attention_type, audio_attention_type)\n",
    "        self.freeze = freeze\n",
    "        \n",
    "    def _build_model(self):\n",
    "        if self.face_attention_type is not \"none\":\n",
    "            self.face_encoder = self.add_attention(self.face_encoder, self.face_attention_type,\n",
    "                                                  {3: 64, 7: 128, 10: 256, 13: 512}) # layer sequence: in_channels\n",
    "            \n",
    "        if self.audio_attention_type is not \"none\":\n",
    "            self.audio_encoder = self.add_attention(self.audio_encoder, self.audio_attention_type,\n",
    "                                                   {2: 32, 5: 64, 8: 128, 11: 256}) # layer sequence, in_channels\n",
    "        \n",
    "    \n",
    "class Syncnet_color_120(_Syncnet_color_Attention):\n",
    "    def __init__(self, face_attention_type=\"none\", audio_attention_type=\"none\", freeze=False):\n",
    "        super(Syncnet_color_120, self).__init__(face_attention_type, audio_attention_type)\n",
    "        self.freeze = freeze\n",
    "            \n",
    "    def _build_model(self):\n",
    "        # add 2x2 conv2d layer\n",
    "        self.face_encoder.add_module(str(len(self.face_encoder)), Conv2d(512, 512, kernel_size=2, stride=1, padding=0)) # 2,2 -> 1,1\n",
    "        \n",
    "        # attention\n",
    "        if self.face_attention_type is not \"none\":\n",
    "            self.face_encoder = self.add_attention(self.face_encoder, self.face_attention_type,\n",
    "                                                  {3: 64, 7: 128, 10: 256, 13: 512}) # layer sequence: in_channels\n",
    "            \n",
    "        if self.audio_attention_type is not \"none\":\n",
    "            self.audio_encoder = self.add_attention(self.audio_encoder, self.audio_attention_type,\n",
    "                                                   {3: 64, 6: 128, 9: 256, 12: 512}) # layer sequence, in_channels\n",
    "        \n",
    "        \n",
    "class Syncnet_color_144(_Syncnet_color_Attention):\n",
    "    def __init__(self, face_attention_type=\"none\", audio_attention_type=\"none\", freeze=False):\n",
    "        super(Syncnet_color_144, self).__init__(face_attention_type, audio_attention_type)\n",
    "        self.freeze = freeze\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # add 2x2 conv2d layer\n",
    "        self.face_encoder.add_module(str(len(self.face_encoder)), Conv2d(512, 512, kernel_size=3, stride=2, padding=1)) # 3,3 -> 1,1\n",
    "        self.face_encoder.add_module(str(len(self.face_encoder)), Conv2d(512, 512, kernel_size=2, stride=1, padding=0)) # 2,2 -> 1,1\n",
    "        \n",
    "        # attention\n",
    "        if self.face_attention_type is not \"none\":\n",
    "            self.face_encoder = self.add_attention(self.face_encoder, self.face_attention_type,\n",
    "                                                  {3: 64, 7: 128, 10: 256, 13: 512}) # layer sequence: in_channels\n",
    "        if self.audio_attention_type is not \"none\":\n",
    "            self.audio_encoder = self.add_attention(self.audio_encoder, self.audio_attention_type,\n",
    "                                                   {3: 64, 6: 128, 9: 256, 12: 512}) # layer sequence, in_channels\n",
    "            \n",
    "        \n",
    "class Syncnet_color_192(_Syncnet_color_Attention):\n",
    "    def __init__(self, face_attention_type=\"none\", audio_attention_type=\"none\", freeze=False):\n",
    "        super(Syncnet_color_192, self).__init__(face_attention_type, audio_attention_type)\n",
    "        self.freeze = freeze\n",
    "\n",
    "    def _build_model(self):\n",
    "        # add module\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            *self.face_encoder[:14], # 96, 192 -> 12, 12\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1), # 6, 6\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            *self.face_encoder[14:] # 1, 1\n",
    "        )\n",
    "        \n",
    "        # attention\n",
    "        if self.face_attention_type is not \"none\":\n",
    "            self.face_encoder = self.add_attention(self.face_encoder, self.face_attention_type,\n",
    "                                                  {3: 64, 7: 128, 10: 256, 13: 512, 16: 512}) # layer sequence: in_channels\n",
    "        if self.audio_attention_type is not \"none\":\n",
    "            self.audio_encoder = self.add_attention(self.audio_encoder, self.audio_attention_type,\n",
    "                                                   {3: 64, 6: 128, 9: 256, 12: 512}) # layer sequence, in_channels\n",
    "            \n",
    "            \n",
    "class Syncnet_color_288(_Syncnet_color_Attention):\n",
    "    def __init__(self, face_attention_type=\"none\", audio_attention_type=\"none\", freeze=False):\n",
    "        super(Syncnet_color_288, self).__init__(face_attention_type, audio_attention_type)\n",
    "        self.freeze = freeze\n",
    "\n",
    "    def _build_model(self):\n",
    "        # add module\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            *self.face_encoder[:14], # 144, 288 -> 18, 18\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1), # 9, 9\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            \n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=2), # 6, 6\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            *self.face_encoder[14:] # 1, 1\n",
    "        )\n",
    "        \n",
    "        # attention\n",
    "        if self.face_attention_type is not \"none\":\n",
    "            self.face_encoder = self.add_attention(self.face_encoder, self.face_attention_type,\n",
    "                                                  {3: 64, 7: 128, 10: 256, 13: 512, 16: 512, 19: 512}) # layer sequence: in_channels\n",
    "        if self.audio_attention_type is not \"none\":\n",
    "            self.audio_encoder = self.add_attention(self.audio_encoder, self.audio_attention_type,\n",
    "                                                   {3: 64, 6: 128, 9: 256, 12: 512}) # layer sequence, in_channels\n",
    "            \n",
    "            \n",
    "def load_syncnet(img_size, face_attention_type, audio_attention_type, **kwargs):\n",
    "    syncnet_dict = {\n",
    "        96: Syncnet_color_96,\n",
    "        120: Syncnet_color_120,\n",
    "        144: Syncnet_color_144,\n",
    "        192: Syncnet_color_192,\n",
    "        288: Syncnet_color_288\n",
    "    }\n",
    "    \n",
    "    syncnet = syncnet_dict.get(img_size, None)\n",
    "    \n",
    "    if syncnet is None:\n",
    "        raise 'img_size {} is not valid.'.format(img_size)\n",
    "        return\n",
    "    \n",
    "    return syncnet(face_attention_type=face_attention_type, audio_attention_type=audio_attention_type, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('mlLearn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa39665c08d819a57db5c2d0163727f65d995a10ca32be95764b9819875e4dc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
